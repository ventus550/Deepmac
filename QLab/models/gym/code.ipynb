{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access parent directories\n",
    "from sys import path\n",
    "from os.path import abspath\n",
    "path += [abspath(\"../\"*i) for i in (1, 2, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from QLab import Qptimizer, ReplayMemory, QNetwork\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(QNetwork):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        self.convolution1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.convolution2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.convolution3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7)\n",
    "        self.fc1 = nn.Linear(in_features=1792, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=9)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.convolution1(x), 3))\n",
    "        x = F.relu(F.max_pool2d(self.convolution2(x), 3))\n",
    "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\tdef __init__(self, qnet : QNetwork, learning_rate = 0.01, gamma = 0.999):\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.memory = ReplayMemory(10000)\n",
    "\n",
    "\t\tself.policy_net = qnet\n",
    "\t\tself.target_net = qnet.clone()\n",
    "\t\tself.target_net.copy_from(self.policy_net)\n",
    "\t\tself.target_net.eval()\n",
    "\n",
    "\t\tself.optimizer = torch.optim.RMSprop(self.policy_net.parameters(), lr=learning_rate)\n",
    "\t\tself.optimizer = Qptimizer(\n",
    "\t\t\tself.memory,\n",
    "\t\t\tself.optimizer,\n",
    "\t\t\tself.policy_net,\n",
    "\t\t\tself.target_net\n",
    "\t\t)\n",
    "\n",
    "\tdef __call__(self, state):\n",
    "\t\taction = utilities.select_epsilon(self.policy_net, state.unsqueeze(0), 0.05) #!\n",
    "\t\tself.perform_action(action)\n",
    "\n",
    "\tdef save(self, path = \"./net\"):\n",
    "\t\tself.policy_net.save(path)\n",
    "\n",
    "\tdef load(self, path = \"./net\"):\n",
    "\t\tself.policy_net.load(path)\n",
    "\n",
    "\tdef feedback(self, state, action, reward, new_state):\n",
    "\t\tself.memory.push(state, action, reward, new_state)\n",
    "\t\tself.optimizer(gamma = self.gamma)\n",
    "\n",
    "\tdef train(self, episodes = 1, update_frq = 10, live = False, plot = False):\n",
    "\n",
    "\t\tscore_history = []\n",
    "\t\tfor episode in tqdm(range(episodes)):\n",
    "\t\t\twhile not env.terminal():\n",
    "\t\t\t\tnext(env)\n",
    "\t\t\t\tif live: env.update()\n",
    "\t\t\tscore_history.append(env.score)\n",
    "\t\t\tif episode % update_frq == 0:\n",
    "\t\t\t\tself.target_net.copy_from(self.policy_net)\n",
    "\t\t\tif plot:\n",
    "\t\t\t\tself.optimizer.plot_loss_variance()\n",
    "\t\t\t\tself.optimizer.plot_loss()\n",
    "\t\t\t\tutilities.quickplot(score_history, \"Score\", \"./score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
